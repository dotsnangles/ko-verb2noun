{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import pandas as pd\n",
    "from jamo import h2j, j2h, j2hcj\n",
    "from konlpy.tag import Kkma, Komoran\n",
    "\n",
    "tokenizer = Komoran(userdic='module/userdic.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "irregular_conjugation_stems = ['걷', '긷', '깨닫', '눋', '닫', '듣', '묻', '붇', '싣', '일컫', '가깝', '가볍', '간지럽', '굽', '그립', '깁', '껄끄럽', '노엽', '더럽', '덥', '맵', '메스껍', '무겁', '반갑', '부끄럽', '사납', '서럽', '쑥스럽', '줍', '긋', '낫', '붓', '잇', '잣', '젓', '짓', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verb_exception_handler(verb):\n",
    "    if verb == '나서':\n",
    "        return '나'\n",
    "    if verb == '아니하':\n",
    "        return '않'\n",
    "    else:\n",
    "        return verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verb_to_noun(verb):\n",
    "    \n",
    "    \"\"\"\n",
    "    한국어의 용언을 명사형으로 변환하는 함수입니다.\n",
    "    용언(동사, 형용사)의 어간을 받아 명사형으로 반환합니다.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    verb = verb_exception_handler(verb)\n",
    "\n",
    "    eum = '음'  # 명사형 전성 어미 '음'\n",
    "    m = h2j('음')[-1]  # 명사형 전성 어미 'ㅁ'\n",
    "\n",
    "    d = h2j('ㄷ')  # ㄷ 불규칙 활용\n",
    "    s = h2j('ㅅ')  # ㅅ 불규칙 활용\n",
    "    b = h2j('ㅂ')  # ㅂ 불규칙 활용\n",
    "\n",
    "    last_syllables = h2j(verb[-1])  # 용언의 마지막 자모군\n",
    "    final = j2hcj(last_syllables[-1])  # 마지막 자모군의 끝글자\n",
    "\n",
    "    pattern = re.compile(r'[ㅏ-ㅣ]')\n",
    "    has_final = not bool(pattern.match(final))  # 마지막 자모군에 종성 존재 여부\n",
    "\n",
    "    if has_final:  # 종성이 존재하는 경우\n",
    "        if final == d and verb in irregular_conjugation_stems:  # ㄷ 불규칙 활용 적용\n",
    "            modified = last_syllables[:-1] + h2j('ㄹ')\n",
    "            return verb[:-1] + j2h(*modified) + eum\n",
    "        elif final == s and verb in irregular_conjugation_stems:  # ㅅ 불규칙 활용 적용\n",
    "            modified = last_syllables[:-1]\n",
    "            return verb[:-1] + j2h(*modified) + eum\n",
    "        if final == b and verb in irregular_conjugation_stems:  # ㅂ 불규칙 활용\n",
    "            modified = h2j('우') + m\n",
    "            return verb[:-1] + j2h(*last_syllables[:-1]) + j2h(*modified)\n",
    "        else:  # 종성이 존재하나 불규칙 활용이 아닌 경우\n",
    "            modified = last_syllables\n",
    "            return verb[:-1] + j2h(*modified) + eum\n",
    "    else:  # 종성이 존재하지 않는 경우\n",
    "        modified = last_syllables + m\n",
    "        return verb[:-1] + j2h(*modified)\n",
    "\n",
    "# 보다 / 돌다 / 걷다 / 긋다 / 가깝다 / 반갑다 / 넘어지\n",
    "\n",
    "# print(verb_to_noun('보'))\n",
    "# print(verb_to_noun('돌'))\n",
    "# print(verb_to_noun('걷'))\n",
    "# print(verb_to_noun('긋'))\n",
    "# print(verb_to_noun('가깝'))\n",
    "# print(verb_to_noun('반갑'))\n",
    "# print(verb_to_noun('넘어지'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(sentence):\n",
    "    \n",
    "    \"\"\"\n",
    "    문장을 입력으로 받아서 형태소 분석을 수행하고, 명사(Noun)와 동사(Verb)를 추출하는 함수\n",
    "    마지막 토큰이 명사형 혹은 동사의 어근이 아닌 경우 삭제합니다.\n",
    "    tokenizer별 형태소 분류표: https://docs.google.com/spreadsheets/d/1OGAjUvalBuX-oZvZ_-9tEfYD2gQe7hTGsgUpiiBSXI8/edit#gid=0\n",
    "\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    \n",
    "    words_w_pos = []\n",
    "    for word in words:\n",
    "        pos = tokenizer.pos(word)\n",
    "        pos = [[word, tag] for word, tag in pos]\n",
    "        words_w_pos.append(pos)\n",
    "\n",
    "    endable_pos = ['VV', 'VA', 'VX', 'NNG', 'NNP']\n",
    "    poppable = False\n",
    "    for word_w_pos in words_w_pos:\n",
    "        for pair in word_w_pos:\n",
    "            if pair[1] in endable_pos:\n",
    "                poppable = True\n",
    "                break\n",
    "        if poppable:\n",
    "            break\n",
    "\n",
    "    if poppable:\n",
    "        for idx in range(len(words_w_pos)-1, -1, -1):\n",
    "            while words_w_pos[idx][-1][1] not in endable_pos:\n",
    "                words_w_pos[idx].pop(-1)\n",
    "                if len(words_w_pos[idx]) < 1:\n",
    "                    break\n",
    "            if len(words_w_pos[idx]) > 0:\n",
    "                break\n",
    "        words_w_pos = [word_w_pos for word_w_pos in words_w_pos if word_w_pos != []]\n",
    "        return words_w_pos\n",
    "    else:\n",
    "        return words_w_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_off_syllable(test_case):\n",
    "    pattern = re.compile(r'[ㄱ-ㅎ]')\n",
    "    searched = pattern.search(test_case)\n",
    "    \n",
    "    if searched:\n",
    "        off_syllable_position, *_ = searched.span()\n",
    "        former_syllables = h2j(test_case[off_syllable_position-1])\n",
    "        if len(former_syllables) < 3:\n",
    "            off_syllable = h2j(test_case[off_syllable_position])\n",
    "            modified = former_syllables + off_syllable\n",
    "            modified = j2h(*modified)\n",
    "            result = test_case[:off_syllable_position-1] + modified + test_case[off_syllable_position+1:]\n",
    "            return result\n",
    "    else:\n",
    "        return test_case\n",
    "    \n",
    "def join_all_off_syllables(test_case):\n",
    "    while True:\n",
    "        result = join_off_syllable(test_case)\n",
    "        if test_case == result:\n",
    "            return result\n",
    "        else:\n",
    "            test_case = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_noun_verb(sentence):\n",
    "\n",
    "    \"\"\"\n",
    "    \"사다리가 쓰러져서\" 문장을 \"사다리가 쓰러짐\" 문자열로 변환하는 함수\n",
    "    extract_keywords를 통해 얻은 keywords_w_pos에 대한 표현 정규화 작업을 수행합니다.\n",
    "    리스트의 마지막 요소가 동사의 어근인 경우만 해당 작업을 수행합니다. (['VV', 'VA', 'VX'])\n",
    "\n",
    "    \"\"\"\n",
    "    words_w_pos = extract_keywords(sentence)\n",
    "\n",
    "    if words_w_pos[-1][-1][1] in ['VV', 'VA', 'VX']:\n",
    "        words_w_pos[-1][-1][0] = verb_to_noun(words_w_pos[-1][-1][0])\n",
    "    \n",
    "    result = [[word for word, tag in word_w_pos] for word_w_pos in words_w_pos]\n",
    "    result = [''.join(el) for el in result]\n",
    "    result = ' '.join(result)\n",
    "    \n",
    "    return  join_all_off_syllables(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_cases = ['발판이 쓰러져서', '사다리가 넘어지다', '사다리가 넘어져서', '사다리가 넘어져', '발판이 쓰러져', '화재 발생', '배탈이 나서', '배탈이 났다']\n",
    "\n",
    "# for item in test_cases:\n",
    "#     result = sentence_to_noun_verb(item)\n",
    "#     print(item, '>>>', result)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_cases = ['결속하지 아니하여', '결속하지 않아' , '착용하지 아니한 채로', '발생하지 않은', '보지 못하고', '보지 않고', '튀어 올라',  '들어가려다가', '결속하지 않아서', '키스토링에 걸리게', '발판이 넘어져서', '발판으로 넘어져서']\n",
    "\n",
    "# for item in test_cases:\n",
    "#     result = sentence_to_noun_verb(item)\n",
    "#     print(item, '>>>', result)\n",
    "#     print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = input('인과 객체 엑셀 파일이 존재하는 폴더명을 입력하세요\\n')\n",
    "print('표현 정규화 작업을 시작합니다.')\n",
    "\n",
    "sheets = sorted([os.path.join(DATA_PATH, sheet) for sheet in os.listdir(DATA_PATH)])\n",
    "col_num_count = lambda sheet: len(pd.read_excel(sheet).columns)\n",
    "col_nums = [col_num_count(sheet) for sheet in sheets]\n",
    "\n",
    "dfs = [pd.read_excel(sheet, names=[f'Col {i}' for i in range(col_num)]) for sheet, col_num in zip(sheets, col_nums)]\n",
    "ce_chains_dfs = [df.iloc[:, 1:] for df in dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_chains_regularized_dfs = [ce_chains.applymap(lambda x: sentence_to_noun_verb(x) if type(x) == str else x) for ce_chains in ce_chains_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sheet, ce_chains_df in zip(sheets, ce_chains_dfs):\n",
    "    file_name = sheet.split('/')[-1].split('.')[0]\n",
    "    ce_chains_df.to_csv(f'./ce_obj_reg_results/{file_name}_original.csv', index=False, encoding='utf-8-sig')\n",
    "    \n",
    "for sheet, ce_chains_regularized_df in zip(sheets, ce_chains_regularized_dfs):\n",
    "    file_name = sheet.split('/')[-1].split('.')[0]\n",
    "    ce_chains_regularized_df.to_csv(f'./ce_obj_reg_results/{file_name}_regularized.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sheet_1 = 'data/eshc 인과관계 학습용(sample)_rev2.xlsx'\n",
    "# sheet_2 = 'data/port 인과관계 학습용(sample)_rev4.xlsx'\n",
    "# data_1 = pd.read_excel(sheet_1, names=[f'Col {i}' for i in range(10)])\n",
    "# data_2 = pd.read_excel(sheet_2, names=[f'Col {i}' for i in range(9)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ce_chains_1 = data_1.iloc[:, 1:]\n",
    "# ce_chains_2 = data_2.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ce_chains_1_regularized = ce_chains_1.applymap(lambda x: sentence_to_noun_verb(x) if type(x) == str else x)\n",
    "# ce_chains_2_regularized = ce_chains_2.applymap(lambda x: sentence_to_noun_verb(x) if type(x) == str else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original = ce_chains_1.values.tolist() + ce_chains_2.values.tolist()\n",
    "# regularized = ce_chains_1_regularized.values.tolist() + ce_chains_2_regularized.values.tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "konlpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "115e57fa8a6cabf4ff14ccb7a0a20905a6bade572e374a3d978bab4e6ffa1af9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
